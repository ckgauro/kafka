# Kafka as a Distributed Streaming System

This guide covers the fundamentals of distributed systems and how Kafka fits into the distributed streaming architecture.

---

## Part 1: What is a Distributed System?

### Definition

A **distributed system** is a collection of independent computers (nodes) that communicate over a network and coordinate their actions to appear as a single coherent system to end users.

Think of it like a restaurant kitchen:
- A single chef (monolith) can only cook so many dishes
- Multiple chefs (distributed) working together can serve hundreds of customers
- They coordinate, share ingredients, and cover for each other if someone is sick

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MONOLITHIC SYSTEM                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                         â”‚                     â”‚                             â”‚
â”‚    User Request â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   SINGLE SERVER     â”‚                             â”‚
â”‚                         â”‚                     â”‚                             â”‚
â”‚                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                             â”‚
â”‚                         â”‚  â”‚  Application  â”‚  â”‚                             â”‚
â”‚                         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                             â”‚
â”‚                         â”‚  â”‚   Database    â”‚  â”‚                             â”‚
â”‚                         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                             â”‚
â”‚                         â”‚  â”‚  File Storage â”‚  â”‚                             â”‚
â”‚                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                             â”‚
â”‚                         â”‚                     â”‚                             â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                                             â”‚
â”‚    PROBLEMS:                                                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚ â€¢ Single Point of Failure - server dies, everything dies      â”‚       â”‚
â”‚    â”‚ â€¢ Limited Scalability - can only add so much CPU/RAM          â”‚       â”‚
â”‚    â”‚ â€¢ Maintenance Downtime - updates require full system restart  â”‚       â”‚
â”‚    â”‚ â€¢ Resource Contention - all processes compete for resources   â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DISTRIBUTED SYSTEM                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                    â”‚      NETWORK / INTERNET      â”‚                         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                       â”‚          â”‚          â”‚                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â–¼                   â–¼                   â–¼                      â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚       â”‚   Node 1   â”‚      â”‚   Node 2   â”‚      â”‚   Node 3   â”‚               â”‚
â”‚       â”‚  (Server)  â”‚â—„â”€â”€â”€â”€â–¶â”‚  (Server)  â”‚â—„â”€â”€â”€â”€â–¶â”‚  (Server)  â”‚               â”‚
â”‚       â”‚            â”‚      â”‚            â”‚      â”‚            â”‚               â”‚
â”‚       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”‚
â”‚       â”‚ â”‚Data: A â”‚ â”‚      â”‚ â”‚Data: B â”‚ â”‚      â”‚ â”‚Data: C â”‚ â”‚               â”‚
â”‚       â”‚ â”‚Copy: B â”‚ â”‚      â”‚ â”‚Copy: C â”‚ â”‚      â”‚ â”‚Copy: A â”‚ â”‚               â”‚
â”‚       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â”‚                   â”‚                   â”‚                      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                    â”‚  Appears as ONE system   â”‚                             â”‚
â”‚                    â”‚     to the end user      â”‚                             â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                                             â”‚
â”‚    BENEFITS:                                                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚ â€¢ No Single Point of Failure - one node down, others continue â”‚       â”‚
â”‚    â”‚ â€¢ Horizontal Scalability - add more nodes as needed           â”‚       â”‚
â”‚    â”‚ â€¢ Geographic Distribution - nodes in different regions        â”‚       â”‚
â”‚    â”‚ â€¢ Parallel Processing - work divided across nodes             â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real-World Examples of Distributed Systems

| System | How It's Distributed |
|--------|---------------------|
| **Google Search** | Query hits one of thousands of servers; index split across data centers worldwide |
| **Netflix** | Content cached on edge servers globally; recommendation engine runs on hundreds of nodes |
| **Amazon** | Orders processed by multiple microservices; inventory synced across warehouses |
| **WhatsApp** | Messages routed through servers in multiple regions; 2M connections per server |
| **Bitcoin** | Every node has a copy of the blockchain; no central authority |

### Why Do We Need Distributed Systems?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHY DISTRIBUTED SYSTEMS?                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. SCALE                                                                   â”‚
â”‚     â”€â”€â”€â”€â”€                                                                   â”‚
â”‚     Modern applications handle:                                             â”‚
â”‚     â€¢ Facebook: 500,000+ requests/second                                    â”‚
â”‚     â€¢ Google: 99,000+ searches/second                                       â”‚
â”‚     â€¢ Kafka at LinkedIn: 7 trillion messages/day                            â”‚
â”‚                                                                             â”‚
â”‚     No single machine can handle this!                                      â”‚
â”‚                                                                             â”‚
â”‚  2. RELIABILITY                                                             â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚
â”‚     Hardware fails. It's not "if" but "when":                               â”‚
â”‚     â€¢ Hard drives: 2-4% annual failure rate                                 â”‚
â”‚     â€¢ Servers: ~5% annual failure rate in large data centers                â”‚
â”‚     â€¢ At 10,000 servers: ~1-2 failures per day!                             â”‚
â”‚                                                                             â”‚
â”‚     We need systems that survive failures.                                  â”‚
â”‚                                                                             â”‚
â”‚  3. LATENCY                                                                 â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€                                                                 â”‚
â”‚     Speed of light limits us:                                               â”‚
â”‚     â€¢ New York to London: ~70ms round trip                                  â”‚
â”‚     â€¢ Users expect <100ms response times                                    â”‚
â”‚                                                                             â”‚
â”‚     Solution: Put servers close to users (CDNs, edge computing)             â”‚
â”‚                                                                             â”‚
â”‚  4. DATA LOCALITY                                                           â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚     Laws like GDPR require data to stay in certain regions                  â”‚
â”‚     â€¢ European user data must stay in EU                                    â”‚
â”‚     â€¢ Distributed systems can enforce this                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Part 1

> "Let's start with a fundamental question: What is a distributed system?
>
> In simple terms, a distributed system is a collection of independent computers that work together and appear as a single system to the end user. Think about the last time you used Netflix or Google. You probably didn't think about it, but behind that simple search or that movie stream, thousands of servers were working together. That's a distributed system.
>
> Let me use a restaurant analogy. Imagine a small restaurant with a single chef. That chef can only cook so many dishes at once. If they get sick, the restaurant closes. If there's a rush, customers wait forever. That's like a monolithic system - everything depends on one server.
>
> Now imagine a restaurant kitchen with ten chefs. They coordinate, share ingredients, specialize in different dishes, and cover for each other if someone is sick. That's a distributed system - multiple nodes working together.
>
> Why do we need them? Three main reasons.
>
> First, scale. No single machine can handle the billions of requests Google gets every day, or the seven trillion messages LinkedIn processes through Kafka daily.
>
> Second, reliability. Hardware fails - it's not IF but WHEN. At the scale of 10,000 servers, you're looking at one or two failures every single day. We need systems that survive these failures automatically.
>
> Third, latency. Physics limits us. Light takes 70 milliseconds just to travel from New York to London. Users expect responses in under 100 milliseconds. The only way to achieve this is to put servers close to users - which means distributing them globally.
>
> Now that we understand what distributed systems are and why we need them, let's look at the qualities that make a distributed system effective."

---

## Part 2: Key Qualities of a Distributed System

### Quality 1: Scalability

**Definition:** The ability of a system to handle increased load by adding resources.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VERTICAL vs HORIZONTAL SCALING                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚    VERTICAL SCALING (Scale Up)          HORIZONTAL SCALING (Scale Out)      â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚                                                                             â”‚
â”‚    Before:        After:                Before:        After:               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”     â”‚
â”‚    â”‚ 4 CPU â”‚      â”‚  32 CPU   â”‚         â”‚Node1â”‚        â”‚Node1â”‚ â”‚Node2â”‚     â”‚
â”‚    â”‚ 8 GB  â”‚ â”€â”€â–¶  â”‚  256 GB   â”‚         â””â”€â”€â”€â”€â”€â”˜   â”€â”€â–¶  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜     â”‚
â”‚    â”‚ 500GB â”‚      â”‚   4 TB    â”‚                        â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚Node3â”‚ â”‚Node4â”‚     â”‚
â”‚                                                        â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                             â”‚
â”‚    PROS:                                PROS:                               â”‚
â”‚    â€¢ Simple - no code changes           â€¢ Unlimited scaling potential       â”‚
â”‚    â€¢ No distributed complexity          â€¢ Better fault tolerance            â”‚
â”‚    â€¢ Lower latency (all local)          â€¢ Cost effective (commodity HW)     â”‚
â”‚                                         â€¢ No downtime to scale              â”‚
â”‚    CONS:                                                                    â”‚
â”‚    â€¢ Hardware limits (can't add         CONS:                               â”‚
â”‚      infinite CPU/RAM)                  â€¢ Distributed system complexity     â”‚
â”‚    â€¢ Expensive high-end hardware        â€¢ Network latency between nodes     â”‚
â”‚    â€¢ Single point of failure            â€¢ Data consistency challenges       â”‚
â”‚    â€¢ Downtime to upgrade                â€¢ More operational overhead         â”‚
â”‚                                                                             â”‚
â”‚    COST CURVE:                                                              â”‚
â”‚                                                                             â”‚
â”‚    $â”‚                    â•± Vertical                                         â”‚
â”‚     â”‚                  â•±   (exponential)                                    â”‚
â”‚     â”‚                â•±                                                      â”‚
â”‚     â”‚              â•±                                                        â”‚
â”‚     â”‚           â•±    â•±â”€â”€â”€â”€â”€â”€â”€ Horizontal                                    â”‚
â”‚     â”‚        â•±   â•±             (linear)                                     â”‚
â”‚     â”‚     â•±  â•±                                                              â”‚
â”‚     â”‚  â•±â•±                                                                   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Capacity                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scalability in Kafka:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KAFKA SCALABILITY                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   SCENARIO: Your application grows from 1,000 to 1,000,000 messages/sec    â”‚
â”‚                                                                             â”‚
â”‚   STEP 1: Add More Partitions                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                                             â”‚
â”‚   Before: Topic with 3 partitions                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚   â”‚   P0    â”‚ â”‚   P1    â”‚ â”‚   P2    â”‚   Max: ~100K msg/sec                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚                                                                             â”‚
â”‚   After: Topic with 30 partitions                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”            â”‚
â”‚   â”‚ P0 â”‚â”‚ P1 â”‚â”‚ P2 â”‚â”‚ P3 â”‚â”‚ P4 â”‚â”‚ P5 â”‚â”‚ P6 â”‚â”‚ P7 â”‚â”‚ P8 â”‚â”‚ P9 â”‚ ...       â”‚
â”‚   â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                             â”‚
â”‚   Max: ~1M msg/sec (10x more partitions = 10x more throughput)             â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   STEP 2: Add More Brokers                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚                                                                             â”‚
â”‚   Before: 3 brokers                      After: 10 brokers                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”    â”‚
â”‚   â”‚Broker 1â”‚â”‚Broker 2â”‚â”‚Broker 3â”‚   â”€â”€â–¶  â”‚ B1 â”‚â”‚ B2 â”‚â”‚ B3 â”‚â”‚ B4 â”‚â”‚ B5 â”‚    â”‚
â”‚   â”‚10 partsâ”‚â”‚10 partsâ”‚â”‚10 partsâ”‚        â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”    â”‚
â”‚                                         â”‚ B6 â”‚â”‚ B7 â”‚â”‚ B8 â”‚â”‚ B9 â”‚â”‚B10 â”‚    â”‚
â”‚                                         â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚   Partitions automatically spread across all brokers                        â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   STEP 3: Add More Consumers                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚                                                                             â”‚
â”‚   Consumer Group with 30 consumers (1 per partition = max parallelism)      â”‚
â”‚                                                                             â”‚
â”‚   P0 â”€â”€â–¶ Consumer 1     P10 â”€â”€â–¶ Consumer 11     P20 â”€â”€â–¶ Consumer 21        â”‚
â”‚   P1 â”€â”€â–¶ Consumer 2     P11 â”€â”€â–¶ Consumer 12     P21 â”€â”€â–¶ Consumer 22        â”‚
â”‚   P2 â”€â”€â–¶ Consumer 3     P12 â”€â”€â–¶ Consumer 13     P22 â”€â”€â–¶ Consumer 23        â”‚
â”‚   ...                   ...                      ...                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Scalability

> "The first quality of a distributed system is scalability - the ability to handle increased load.
>
> There are two ways to scale: vertical and horizontal.
>
> Vertical scaling, or scaling up, means getting a bigger machine - more CPU, more RAM, more storage. It's simple because you don't need to change your code. But it has limits. You can't add infinite resources to a single machine, and high-end hardware gets exponentially more expensive. Plus, it's still a single point of failure.
>
> Horizontal scaling, or scaling out, means adding more machines. This is what distributed systems do. The cost grows linearly - ten small servers often cost less than one giant server with equivalent power. And there's virtually no limit to how far you can scale.
>
> Let's see how Kafka scales. Say your application grows from a thousand messages per second to a million.
>
> First, you add more partitions. Partitions are Kafka's unit of parallelism. A topic with three partitions might handle 100,000 messages per second. Increase to thirty partitions, and you can handle a million.
>
> Second, you add more brokers. Kafka automatically spreads partitions across available brokers. More brokers means more disk I/O capacity, more network bandwidth, more CPU for handling requests.
>
> Third, you add more consumers. With thirty partitions, you can have up to thirty consumers in a group, each processing one partition in parallel.
>
> This is horizontal scalability in action. No downtime, no code changes - just add resources and Kafka adapts."

---

### Quality 2: Fault Tolerance

**Definition:** The ability of a system to continue operating properly in the event of failure of some of its components.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TYPES OF FAILURES IN DISTRIBUTED SYSTEMS                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   1. CRASH FAILURES                                                         â”‚
â”‚      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚      Node stops working completely (hardware failure, power outage)         â”‚
â”‚                                                                             â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚      â”‚ Node 1  â”‚      â”‚ Node 2  â”‚      â”‚ Node 3  â”‚                         â”‚
â”‚      â”‚   âœ“     â”‚      â”‚   ğŸ’€    â”‚      â”‚   âœ“     â”‚                         â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                        (crashed)                                            â”‚
â”‚                                                                             â”‚
â”‚   2. NETWORK FAILURES                                                       â”‚
â”‚      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚      Nodes can't communicate (cable cut, switch failure, congestion)        â”‚
â”‚                                                                             â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚      â”‚ Node 1  â”‚â”€â”€â”€â”€â”€â”€â•³â”€â”€â”€â”€â”€â”€â”€â”‚ Node 2  â”‚                                   â”‚
â”‚      â”‚   âœ“     â”‚   (network   â”‚   âœ“     â”‚                                   â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    broken)   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                                                             â”‚
â”‚   3. BYZANTINE FAILURES (Malicious/Corrupted)                               â”‚
â”‚      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚      Node behaves incorrectly (sends wrong data, lies about state)          â”‚
â”‚                                                                             â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚      â”‚ Node 1  â”‚      â”‚ Node 2  â”‚      â”‚ Node 3  â”‚                         â”‚
â”‚      â”‚  "A=5"  â”‚      â”‚  "A=5"  â”‚      â”‚  "A=99" â”‚ â† Lying!                â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                             â”‚
â”‚   4. PERFORMANCE FAILURES                                                   â”‚
â”‚      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚      Node responds too slowly (overloaded, garbage collection pause)        â”‚
â”‚                                                                             â”‚
â”‚      Request â”€â”€â–¶ Node â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Response (10 sec later)   â”‚
â”‚                       (expected: 100ms)                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How Kafka Handles Failures:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA FAULT TOLERANCE MECHANISMS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   MECHANISM 1: REPLICATION                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚                                                                             â”‚
â”‚   Topic: orders, Partition: 0, Replication Factor: 3                        â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚    Broker 1     â”‚  â”‚    Broker 2     â”‚  â”‚    Broker 3     â”‚            â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚   â”‚  â”‚ orders-P0 â”‚  â”‚  â”‚  â”‚ orders-P0 â”‚  â”‚  â”‚  â”‚ orders-P0 â”‚  â”‚            â”‚
â”‚   â”‚  â”‚  LEADER   â”‚â”€â”€â”¼â”€â”€â”¼â”€â–¶â”‚  FOLLOWER â”‚  â”‚  â”‚  â”‚  FOLLOWER â”‚  â”‚            â”‚
â”‚   â”‚  â”‚           â”‚â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â–¶â”‚           â”‚  â”‚            â”‚
â”‚   â”‚  â”‚  [1,2,3]  â”‚  â”‚  â”‚  â”‚  [1,2,3]  â”‚  â”‚  â”‚  â”‚  [1,2,3]  â”‚  â”‚            â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                             â”‚
â”‚   All 3 copies have the same data: messages [1, 2, 3]                       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   MECHANISM 2: LEADER ELECTION                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                                             â”‚
â”‚   What happens when the leader fails?                                       â”‚
â”‚                                                                             â”‚
â”‚   Before:                           After:                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚   â”‚  Broker 1  â”‚                    â”‚  Broker 1  â”‚                         â”‚
â”‚   â”‚  LEADER ğŸ’€ â”‚ â”€â”€ fails â”€â”€â–¶      â”‚   (dead)   â”‚                         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚   â”‚  Broker 2  â”‚                    â”‚  Broker 2  â”‚                         â”‚
â”‚   â”‚  FOLLOWER  â”‚ â”€â”€ promoted â”€â”€â–¶   â”‚ NEW LEADER â”‚ â—€â”€â”€ Producers/Consumers â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      reconnect here     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚   â”‚  Broker 3  â”‚                    â”‚  Broker 3  â”‚                         â”‚
â”‚   â”‚  FOLLOWER  â”‚                    â”‚  FOLLOWER  â”‚                         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                             â”‚
â”‚   Automatic! No manual intervention needed.                                 â”‚
â”‚   Typically takes < 1 second with KRaft.                                    â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   MECHANISM 3: IN-SYNC REPLICAS (ISR)                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚                                                                             â”‚
â”‚   ISR = Set of replicas that are fully caught up with the leader            â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  Leader: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]               â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  Follower A: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  âœ“ IN ISR â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  Follower B: [1, 2, 3, 4, 5, 6, 7]  âœ— OUT OF ISR       â”‚              â”‚
â”‚   â”‚              (lagging behind - maybe slow network)      â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                             â”‚
â”‚   Only ISR members can become leader (they have all the data)               â”‚
â”‚                                                                             â”‚
â”‚   Config: min.insync.replicas = 2                                           â”‚
â”‚   Meaning: At least 2 replicas must acknowledge before commit               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Fault Tolerance

> "The second quality is fault tolerance - the system's ability to keep working even when things break.
>
> In distributed systems, we deal with several types of failures. Crash failures - a server simply dies due to hardware failure or power outage. Network failures - servers are alive but can't communicate because a cable was cut or a switch failed. Byzantine failures - a server behaves incorrectly, maybe due to a bug or malicious actor. And performance failures - a server responds, but way too slowly.
>
> Kafka handles these through three mechanisms.
>
> First, replication. Every partition has multiple copies spread across different brokers. With a replication factor of three, your data exists on three separate machines. If one dies, two others still have it.
>
> Second, automatic leader election. Each partition has one leader that handles all reads and writes. Followers just replicate. When the leader fails, Kafka automatically promotes a follower to become the new leader. With KRaft, this happens in under a second. Producers and consumers briefly reconnect and continue working.
>
> Third, In-Sync Replicas, or ISR. Kafka tracks which replicas are fully caught up with the leader. Only ISR members can become the new leader - this ensures no data loss during failover. The min.insync.replicas setting lets you configure how many replicas must acknowledge a write before it's considered committed.
>
> Together, these mechanisms mean Kafka can survive broker failures, disk failures, and even entire data center outages without losing data."

---

### Quality 3: High Availability

**Definition:** The system remains operational and accessible for a very high percentage of time.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AVAILABILITY LEVELS (The "Nines")                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Availability â”‚ Downtime/Year  â”‚ Downtime/Month â”‚ Downtime/Week           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚      90%       â”‚   36.5 days    â”‚    3 days      â”‚   16.8 hours            â”‚
â”‚      99%       â”‚   3.65 days    â”‚    7.3 hours   â”‚   1.68 hours            â”‚
â”‚      99.9%     â”‚   8.76 hours   â”‚   43.8 minutes â”‚   10.1 minutes          â”‚
â”‚      99.99%    â”‚   52.6 minutes â”‚   4.4 minutes  â”‚   1.01 minutes          â”‚
â”‚      99.999%   â”‚   5.26 minutes â”‚   26.3 seconds â”‚   6.05 seconds          â”‚
â”‚      99.9999%  â”‚   31.5 seconds â”‚   2.6 seconds  â”‚   0.6 seconds           â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚  REAL-WORLD EXAMPLES:                                           â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  â€¢ Most SaaS products target: 99.9% (three nines)              â”‚       â”‚
â”‚   â”‚  â€¢ Cloud providers (AWS, GCP): 99.99% for compute              â”‚       â”‚
â”‚   â”‚  â€¢ Banking/Financial: 99.999% (five nines)                     â”‚       â”‚
â”‚   â”‚  â€¢ Air Traffic Control: 99.9999% (six nines)                   â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚   AVAILABILITY FORMULA:                                                     â”‚
â”‚                                                                             â”‚
â”‚                         MTBF                                                â”‚
â”‚   Availability = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚                     MTBF + MTTR                                             â”‚
â”‚                                                                             â”‚
â”‚   MTBF = Mean Time Between Failures                                         â”‚
â”‚   MTTR = Mean Time To Recovery                                              â”‚
â”‚                                                                             â”‚
â”‚   To increase availability:                                                 â”‚
â”‚   â€¢ Increase MTBF (better hardware, redundancy)                             â”‚
â”‚   â€¢ Decrease MTTR (faster detection, automatic failover)                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Kafka High Availability:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA HIGH AVAILABILITY SETUP                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   MULTI-DATACENTER DEPLOYMENT                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚                                                                             â”‚
â”‚        Data Center A (US-East)              Data Center B (US-West)         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”‚     â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚
â”‚   â”‚  â”‚Broker1â”‚  â”‚Broker2â”‚      â”‚     â”‚      â”‚Broker4â”‚  â”‚Broker5â”‚  â”‚      â”‚
â”‚   â”‚  â”‚ P0(L) â”‚  â”‚ P1(L) â”‚      â”‚â—„â”€â”€â”€â–¶â”‚      â”‚ P0(F) â”‚  â”‚ P1(F) â”‚  â”‚      â”‚
â”‚   â”‚  â”‚ P1(F) â”‚  â”‚ P0(F) â”‚      â”‚     â”‚      â”‚ P2(L) â”‚  â”‚ P2(F) â”‚  â”‚      â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚
â”‚   â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”‚     â”‚                            â”‚      â”‚
â”‚   â”‚        â”‚Broker3â”‚           â”‚     â”‚                            â”‚      â”‚
â”‚   â”‚        â”‚ P2(F) â”‚           â”‚     â”‚                            â”‚      â”‚
â”‚   â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚     â”‚                            â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â”‚   L = Leader, F = Follower                                                  â”‚
â”‚                                                                             â”‚
â”‚   FAILURE SCENARIOS:                                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ Scenario           â”‚ Impact                â”‚ Recovery Time      â”‚       â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚   â”‚ Single broker dies â”‚ None (auto-failover)  â”‚ < 1 second         â”‚       â”‚
â”‚   â”‚ Entire DC-A fails  â”‚ Brief pause           â”‚ ~30 seconds        â”‚       â”‚
â”‚   â”‚ Network partition  â”‚ Reduced capacity      â”‚ Continues serving  â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚   KEY CONFIGURATIONS FOR HA:                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚                                                                             â”‚
â”‚   replication.factor = 3                                                    â”‚
â”‚   min.insync.replicas = 2                                                   â”‚
â”‚   acks = all                                                                â”‚
â”‚   unclean.leader.election.enable = false                                    â”‚
â”‚                                                                             â”‚
â”‚   This ensures:                                                             â”‚
â”‚   â€¢ Data exists on 3 brokers                                                â”‚
â”‚   â€¢ At least 2 must acknowledge writes                                      â”‚
â”‚   â€¢ Producer waits for all in-sync replicas                                 â”‚
â”‚   â€¢ Only fully-synced replicas can become leader                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - High Availability

> "High availability is about keeping the system accessible. We measure it in nines - 99.9% availability, called three nines, means about 8 hours of downtime per year. Five nines - 99.999% - means only about 5 minutes of downtime per year.
>
> The formula is simple: availability equals mean time between failures divided by mean time between failures plus mean time to recovery. To improve availability, you either make failures less frequent or recover from them faster.
>
> Kafka achieves high availability through its distributed architecture. With brokers spread across multiple data centers, a single broker failure has zero impact - automatic failover happens in under a second. Even if an entire data center fails, Kafka continues serving from the surviving data center.
>
> The key configurations for high availability are: replication factor of three to ensure data exists on multiple brokers, min.insync.replicas of two to guarantee durability, acks set to all so producers wait for confirmation, and unclean leader election disabled to prevent data loss.
>
> With proper configuration, Kafka can achieve five nines of availability - that's less than five minutes of downtime per year."

---

### Quality 4: Partition Tolerance

**Definition:** The system continues to operate despite network partitions (when nodes can't communicate with each other).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NETWORK PARTITION EXPLAINED                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   NORMAL STATE:                                                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚ Node A  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Node B  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Node C  â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚        â”‚                                       â”‚                            â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚              All nodes can communicate                                      â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   NETWORK PARTITION:                                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•³    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚ Node A  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Node B  â”‚â—„â”€â”€â”€â•³â”€â”€â”€â–¶â”‚ Node C  â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•³    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚        â”‚                             â•³                                      â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•³â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                      â•³                                      â”‚
â”‚        PARTITION 1                        PARTITION 2                       â”‚
â”‚      (Nodes A and B)                      (Node C alone)                    â”‚
â”‚                                                                             â”‚
â”‚   Common causes:                                                            â”‚
â”‚   â€¢ Firewall misconfiguration                                               â”‚
â”‚   â€¢ Switch/router failure                                                   â”‚
â”‚   â€¢ Undersea cable cut                                                      â”‚
â”‚   â€¢ Cloud provider network issues                                           â”‚
â”‚   â€¢ DDoS attacks on network infrastructure                                  â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   THE SPLIT-BRAIN PROBLEM:                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚                                                                             â”‚
â”‚   Both partitions think they're the "real" cluster                          â”‚
â”‚                                                                             â”‚
â”‚   Partition 1:              Partition 2:                                    â”‚
â”‚   "I have 2 nodes,          "I'm still alive,                               â”‚
â”‚    I'll keep serving"        I'll keep serving"                             â”‚
â”‚                                                                             â”‚
â”‚   Client A writes X=5 â”€â”€â–¶ Partition 1                                       â”‚
â”‚   Client B writes X=10 â”€â”€â–¶ Partition 2                                      â”‚
â”‚                                                                             â”‚
â”‚   When network heals: X=5 or X=10? DATA INCONSISTENCY!                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How Kafka Handles Partitions:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA PARTITION TOLERANCE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   KAFKA'S APPROACH: Quorum-based consensus (KRaft uses Raft protocol)       â”‚
â”‚                                                                             â”‚
â”‚   SCENARIO: 5 Broker Cluster with Network Partition                         â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚      PARTITION A            â”‚   â•³   â”‚       PARTITION B           â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”‚   â•³   â”‚   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚   â”‚  â”‚ B1  â”‚ â”‚ B2  â”‚ â”‚ B3  â”‚   â”‚   â•³   â”‚   â”‚ B4  â”‚ â”‚ B5  â”‚          â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â”‚   â•³   â”‚   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚   â”‚     (3 brokers = MAJORITY)  â”‚   â•³   â”‚   (2 brokers = MINORITY)   â”‚    â”‚
â”‚   â”‚           âœ“ ACTIVE          â”‚   â•³   â”‚       âœ— INACTIVE           â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•³   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚   QUORUM RULE: Majority (N/2 + 1) must agree                                â”‚
â”‚                                                                             â”‚
â”‚   5 brokers â†’ need 3 to agree (Partition A has 3, so it's the leader)       â”‚
â”‚   Partition B (only 2) cannot form quorum â†’ becomes read-only or stops      â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   WHY THIS WORKS:                                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚                                                                             â”‚
â”‚   â€¢ Only ONE partition can have majority                                    â”‚
â”‚   â€¢ Prevents split-brain (both sides can't accept writes)                   â”‚
â”‚   â€¢ When network heals, minority rejoins and syncs                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Partition Tolerance

> "Partition tolerance is about handling network failures - when nodes in your system can't communicate with each other.
>
> This happens more often than you'd think. A misconfigured firewall, a failed switch, an undersea cable cut, or even a cloud provider having issues. Suddenly your cluster is split into groups that can't talk to each other.
>
> This creates the split-brain problem. Imagine your cluster splits into two halves. Both halves think they're the real cluster. A client writes X equals 5 to one half, another client writes X equals 10 to the other half. When the network heals, which value is correct? You have data inconsistency.
>
> Kafka solves this with quorum-based consensus using the Raft protocol in KRaft mode. The rule is simple: a majority of nodes must agree for any decision.
>
> In a five-broker cluster, you need three brokers to agree. If a network partition splits your cluster into groups of three and two, only the group with three can continue accepting writes. The group with two knows it doesn't have a majority, so it stops accepting writes to prevent inconsistency.
>
> When the network heals, the minority side simply rejoins and syncs up with what it missed. No conflicts, no data loss, no inconsistency."

---

### Quality 5: Consistency

**Definition:** All nodes see the same data at the same time.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSISTENCY MODELS                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   STRONG CONSISTENCY                                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚   After a write completes, ALL subsequent reads return that value.          â”‚
â”‚                                                                             â”‚
â”‚   Time â”€â”€â–¶                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚ Client A:  Write X=5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Success         â”‚           â”‚
â”‚   â”‚                                            â”‚                â”‚           â”‚
â”‚   â”‚ Client B:                                  â”‚ Read X â”€â”€â–¶ 5   â”‚           â”‚
â”‚   â”‚ Client C:                                  â”‚ Read X â”€â”€â–¶ 5   â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                             â”‚
â”‚   PROS: Predictable, easy to reason about                                   â”‚
â”‚   CONS: Slower (must wait for all nodes), less available                    â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   EVENTUAL CONSISTENCY                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚   After a write, reads MAY return old value temporarily.                    â”‚
â”‚   Eventually (usually milliseconds), all reads return new value.            â”‚
â”‚                                                                             â”‚
â”‚   Time â”€â”€â–¶                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚ Client A:  Write X=5 â”€â”€â–¶ Success                            â”‚           â”‚
â”‚   â”‚                         â”‚                                   â”‚           â”‚
â”‚   â”‚ Client B:               â”‚ Read X â”€â”€â–¶ 3 (old!)               â”‚           â”‚
â”‚   â”‚ Client C:               â”‚        Read X â”€â”€â–¶ 3 (old!)        â”‚           â”‚
â”‚   â”‚                         â”‚                   â”‚               â”‚           â”‚
â”‚   â”‚                         â”‚ â•â•â• Sync happens â•â•â•              â”‚           â”‚
â”‚   â”‚                         â”‚                       â”‚           â”‚           â”‚
â”‚   â”‚ Client B:               â”‚                  Read X â”€â”€â–¶ 5 âœ“   â”‚           â”‚
â”‚   â”‚ Client C:               â”‚                  Read X â”€â”€â–¶ 5 âœ“   â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                             â”‚
â”‚   PROS: Faster, more available                                              â”‚
â”‚   CONS: Temporary inconsistency, harder to reason about                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Kafka Consistency Options:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA CONSISTENCY - THE "acks" SETTING                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   The producer's "acks" configuration determines consistency vs speed       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   acks = 0 (Fire and Forget)                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚                                                                             â”‚
â”‚   Producer â”€â”€â–¶ Message â”€â”€â–¶ Broker                                           â”‚
â”‚       â”‚                       â”‚                                             â”‚
â”‚       â”‚ (doesn't wait)        â”‚ (might fail)                                â”‚
â”‚       â–¼                       â–¼                                             â”‚
â”‚   Continue immediately     Maybe written, maybe not                         â”‚
â”‚                                                                             â”‚
â”‚   SPEED: â˜…â˜…â˜…â˜…â˜…  |  DURABILITY: â˜…â˜†â˜†â˜†â˜†  |  USE CASE: Metrics, logs           â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   acks = 1 (Leader Acknowledgment)                                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚                                                                             â”‚
â”‚   Producer â”€â”€â–¶ Message â”€â”€â–¶ Leader Broker â”€â”€â–¶ Write to disk                  â”‚
â”‚       â”‚                         â”‚                â”‚                          â”‚
â”‚       â”‚â—€â”€â”€â”€â”€â”€â”€ Ack â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚                          â”‚
â”‚       â”‚                                          â”‚                          â”‚
â”‚       â–¼                                          â–¼                          â”‚
â”‚   Continue                              Replicas sync LATER                 â”‚
â”‚                                         (might lose if leader dies)         â”‚
â”‚                                                                             â”‚
â”‚   SPEED: â˜…â˜…â˜…â˜…â˜†  |  DURABILITY: â˜…â˜…â˜…â˜†â˜†  |  USE CASE: General purpose         â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   acks = all (Full Replication)                                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚                                                                             â”‚
â”‚   Producer â”€â”€â–¶ Message â”€â”€â–¶ Leader â”€â”€â–¶ Replicas                              â”‚
â”‚       â”‚                       â”‚           â”‚                                 â”‚
â”‚       â”‚                       â”‚â—€â”€â”€ Sync â”€â”€â”˜                                 â”‚
â”‚       â”‚                       â”‚                                             â”‚
â”‚       â”‚â—€â”€â”€â”€â”€â”€â”€ Ack â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚       â”‚      (after ALL ISR replicas confirm)                               â”‚
â”‚       â–¼                                                                     â”‚
â”‚   Continue (data is SAFE)                                                   â”‚
â”‚                                                                             â”‚
â”‚   SPEED: â˜…â˜…â˜…â˜†â˜†  |  DURABILITY: â˜…â˜…â˜…â˜…â˜…  |  USE CASE: Financial, orders       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   SUMMARY TABLE:                                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚   acks    â”‚   Latency    â”‚   Durability  â”‚   Risk                  â”‚    â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    â”‚
â”‚   â”‚    0      â”‚   Lowest     â”‚   None        â”‚   Message loss common   â”‚    â”‚
â”‚   â”‚    1      â”‚   Low        â”‚   Medium      â”‚   Loss if leader fails  â”‚    â”‚
â”‚   â”‚   all     â”‚   Higher     â”‚   Highest     â”‚   Minimal (ISR fails)   â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Consistency

> "The final quality is consistency - ensuring all nodes see the same data at the same time.
>
> There are two main models. Strong consistency means after a write completes, all subsequent reads see that write. It's predictable and easy to reason about, but slower because you have to wait for all nodes to sync.
>
> Eventual consistency means reads might temporarily return old data, but will eventually return the new value. It's faster and more available, but you have to handle temporary inconsistency in your application.
>
> Kafka lets you choose your consistency level through the acks setting.
>
> With acks equals zero, fire and forget - the producer doesn't wait for any acknowledgment. Maximum speed, but messages might be lost. Good for metrics or logs where losing a few data points is acceptable.
>
> With acks equals one, the producer waits for the leader to acknowledge. If the leader crashes before replicating, you lose the message. This is a good balance for most use cases.
>
> With acks equals all, the producer waits for all in-sync replicas to acknowledge. This is strong consistency - if you get an acknowledgment, your data is safe on multiple brokers. Use this for financial transactions, orders, or anything where data loss is unacceptable.
>
> The choice is yours based on your use case. Kafka gives you the flexibility to trade off consistency for performance as needed."

---

## Part 3: The CAP Theorem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE CAP THEOREM                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Proposed by Eric Brewer (2000), proven by Gilbert & Lynch (2002)          â”‚
â”‚                                                                             â”‚
â”‚   In a distributed system, you can only guarantee TWO of THREE:             â”‚
â”‚                                                                             â”‚
â”‚                          CONSISTENCY                                        â”‚
â”‚                              â–²                                              â”‚
â”‚                             /â”‚\                                             â”‚
â”‚                            / â”‚ \                                            â”‚
â”‚                           /  â”‚  \                                           â”‚
â”‚                          /   â”‚   \                                          â”‚
â”‚                         /    â”‚    \                                         â”‚
â”‚                        /  CA â”‚     \                                        â”‚
â”‚                       /  (notâ”‚real) \                                       â”‚
â”‚                      /   â”€â”€â”€â”€â”¼â”€â”€â”€â”€   \                                      â”‚
â”‚                     /        â”‚        \                                     â”‚
â”‚                    /   CP    â”‚    AP   \                                    â”‚
â”‚                   /  Systems â”‚ Systems  \                                   â”‚
â”‚                  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\                                  â”‚
â”‚                 â–¼            â”‚            â–¼                                 â”‚
â”‚           PARTITION â—€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â–¶ AVAILABILITY                         â”‚
â”‚           TOLERANCE                                                         â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   WHAT DOES EACH MEAN?                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚                                                                             â”‚
â”‚   C (Consistency):                                                          â”‚
â”‚      Every read returns the most recent write.                              â”‚
â”‚      "All nodes see the same data at the same time"                         â”‚
â”‚                                                                             â”‚
â”‚   A (Availability):                                                         â”‚
â”‚      Every request receives a response (success or failure).                â”‚
â”‚      "The system is always operational"                                     â”‚
â”‚                                                                             â”‚
â”‚   P (Partition Tolerance):                                                  â”‚
â”‚      The system operates despite network partitions.                        â”‚
â”‚      "Works even when nodes can't talk to each other"                       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   WHY CAN'T WE HAVE ALL THREE?                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                                             â”‚
â”‚   Imagine a network partition:                                              â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â•³          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚  Node 1   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â•³â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Node 2   â”‚                          â”‚
â”‚   â”‚   X = 5   â”‚          â•³          â”‚   X = 5   â”‚                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â•³          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                             â”‚
â”‚   Client wants to write X = 10 to Node 1:                                   â”‚
â”‚                                                                             â”‚
â”‚   OPTION 1 (Choose Availability):                                           â”‚
â”‚   â€¢ Accept the write on Node 1                                              â”‚
â”‚   â€¢ Node 1 has X=10, Node 2 has X=5  â”€â”€â–¶ INCONSISTENT!                      â”‚
â”‚                                                                             â”‚
â”‚   OPTION 2 (Choose Consistency):                                            â”‚
â”‚   â€¢ Reject the write (can't sync with Node 2)                               â”‚
â”‚   â€¢ System is unavailable for writes â”€â”€â–¶ NOT AVAILABLE!                     â”‚
â”‚                                                                             â”‚
â”‚   You MUST choose. This is the CAP theorem in action.                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Where Different Systems Fall

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAP THEOREM - REAL SYSTEMS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   CP SYSTEMS (Consistency + Partition Tolerance)                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚   Sacrifice: Availability (may reject requests during partitions)           â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ System        â”‚ How It Works                                    â”‚       â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚   â”‚ Apache Kafka  â”‚ Waits for ISR acknowledgment; may reject writes â”‚       â”‚
â”‚   â”‚ MongoDB       â”‚ Primary must reach majority for writes          â”‚       â”‚
â”‚   â”‚ Redis Cluster â”‚ Majority of masters must be reachable           â”‚       â”‚
â”‚   â”‚ Zookeeper     â”‚ Quorum needed for any operation                 â”‚       â”‚
â”‚   â”‚ etcd          â”‚ Raft consensus requires majority                â”‚       â”‚
â”‚   â”‚ HBase         â”‚ Strong consistency, may become unavailable      â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚   USE CASES: Banking, inventory, anything where wrong data is worse         â”‚
â”‚              than no data                                                   â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   AP SYSTEMS (Availability + Partition Tolerance)                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚   Sacrifice: Consistency (may return stale data during partitions)          â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ System        â”‚ How It Works                                    â”‚       â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚   â”‚ Cassandra     â”‚ Always accepts writes; resolves conflicts later â”‚       â”‚
â”‚   â”‚ DynamoDB      â”‚ Eventual consistency by default                 â”‚       â”‚
â”‚   â”‚ CouchDB       â”‚ Multi-master; syncs when possible               â”‚       â”‚
â”‚   â”‚ Riak          â”‚ Vector clocks for conflict resolution           â”‚       â”‚
â”‚   â”‚ DNS           â”‚ Caches may serve stale data                     â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚   USE CASES: Shopping carts, social media feeds, anything where             â”‚
â”‚              availability matters more than perfect accuracy                â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   CA SYSTEMS (Consistency + Availability)                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚   Sacrifice: Partition Tolerance (can't handle network failures)            â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ System           â”‚ Reality                                      â”‚       â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚   â”‚ Single-node RDBMSâ”‚ MySQL/Postgres on one server                 â”‚       â”‚
â”‚   â”‚                  â”‚ (Not distributed - P doesn't apply)          â”‚       â”‚
â”‚   â”‚                  â”‚                                              â”‚       â”‚
â”‚   â”‚ In theory...     â”‚ CA doesn't exist in distributed systems!     â”‚       â”‚
â”‚   â”‚                  â”‚ Networks ALWAYS can fail.                    â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚   IMPORTANT: In real distributed systems, P is non-negotiable.              â”‚
â”‚   Networks WILL fail. So the real choice is between CP and AP.              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - CAP Theorem

> "Now let's talk about the CAP theorem - one of the most important concepts in distributed systems.
>
> Proposed by Eric Brewer in 2000 and formally proven in 2002, the CAP theorem states that in a distributed system, you can only guarantee two of three properties: Consistency, Availability, and Partition Tolerance.
>
> Consistency means every read returns the most recent write - all nodes see the same data. Availability means every request gets a response - the system is always operational. Partition Tolerance means the system works even when nodes can't communicate due to network failures.
>
> Here's why you can't have all three. Imagine two nodes with a network partition between them. A client wants to write X equals 10 to Node 1. You have two options.
>
> Option one: accept the write. Node 1 has X equals 10, but Node 2 still has the old value. You've sacrificed consistency for availability.
>
> Option two: reject the write because you can't sync with Node 2. You've sacrificed availability for consistency.
>
> There's no third option. This is the fundamental tradeoff.
>
> In practice, partition tolerance is non-negotiable - networks always fail eventually. So the real choice is between CP and AP systems.
>
> Kafka is a CP system. It prioritizes consistency over availability. With acks equals all and min.insync.replicas configured, Kafka will reject writes rather than risk inconsistency. This is the right choice when you're processing financial transactions or tracking inventory - wrong data is worse than no data."

---

## Part 4: How Kafka Fits as a Distributed System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA'S DISTRIBUTED ARCHITECTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚                           KAFKA CLUSTER                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚   â”‚    â”‚  Broker 1   â”‚    â”‚  Broker 2   â”‚    â”‚  Broker 3   â”‚           â”‚   â”‚
â”‚   â”‚    â”‚  (node.id=1)â”‚    â”‚  (node.id=2)â”‚    â”‚  (node.id=3)â”‚           â”‚   â”‚
â”‚   â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”‚orders-P0â”‚ â”‚    â”‚ â”‚orders-P0â”‚ â”‚    â”‚ â”‚orders-P0â”‚ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”‚ LEADER  â”‚â—€â”¼â”€â”€â”€â”€â”¼â–¶â”‚ REPLICA â”‚â—€â”¼â”€â”€â”€â”€â”¼â–¶â”‚ REPLICA â”‚ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”‚orders-P1â”‚ â”‚    â”‚ â”‚orders-P1â”‚ â”‚    â”‚ â”‚orders-P1â”‚ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”‚ REPLICA â”‚â—€â”¼â”€â”€â”€â”€â”¼â–¶â”‚ LEADER  â”‚â—€â”¼â”€â”€â”€â”€â”¼â–¶â”‚ REPLICA â”‚ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”‚orders-P2â”‚ â”‚    â”‚ â”‚orders-P2â”‚ â”‚    â”‚ â”‚orders-P2â”‚ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â”‚ REPLICA â”‚â—€â”¼â”€â”€â”€â”€â”¼â–¶â”‚ REPLICA â”‚â—€â”¼â”€â”€â”€â”€â”¼â–¶â”‚ LEADER  â”‚ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚           â”‚   â”‚
â”‚   â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚           â”‚   â”‚
â”‚   â”‚    â”‚ Controller  â”‚    â”‚             â”‚    â”‚             â”‚           â”‚   â”‚
â”‚   â”‚    â”‚  (active)   â”‚    â”‚             â”‚    â”‚             â”‚           â”‚   â”‚
â”‚   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   KEY DISTRIBUTED COMPONENTS:                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                                             â”‚
â”‚   â€¢ BROKERS: Independent servers that store and serve data                  â”‚
â”‚   â€¢ PARTITIONS: Data split across brokers for parallelism                   â”‚
â”‚   â€¢ REPLICAS: Copies of partitions on different brokers for fault tolerance â”‚
â”‚   â€¢ CONTROLLER: Manages cluster state, leader elections (KRaft)             â”‚
â”‚   â€¢ PRODUCERS: Write data to partition leaders                              â”‚
â”‚   â€¢ CONSUMERS: Read data from partition leaders (or followers in 2.4+)      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Kafka's Implementation of Distributed Qualities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA - DISTRIBUTED QUALITIES MAPPING                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•— â”‚
â”‚   â•‘  QUALITY         â•‘  KAFKA IMPLEMENTATION                              â•‘ â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â•‘  SCALABILITY     â•‘  â€¢ Partitions: Divide data across brokers          â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ Add brokers: Kafka auto-rebalances partitions   â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ Consumer groups: Parallel processing            â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ Benchmark: 2M+ messages/sec per broker          â•‘ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â•‘  FAULT           â•‘  â€¢ Replication: Each partition has N copies        â•‘ â”‚
â”‚   â•‘  TOLERANCE       â•‘  â€¢ ISR: In-Sync Replicas track healthy copies      â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ Leader election: Automatic failover             â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ min.insync.replicas: Guarantee durability       â•‘ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â•‘  HIGH            â•‘  â€¢ Multi-broker: No single point of failure        â•‘ â”‚
â”‚   â•‘  AVAILABILITY    â•‘  â€¢ Controller failover: KRaft quorum election      â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ Multi-datacenter: MirrorMaker 2 replication     â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ Recovery time: <1 second for leader election    â•‘ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â•‘  PARTITION       â•‘  â€¢ Quorum: Majority needed for consensus           â•‘ â”‚
â”‚   â•‘  TOLERANCE       â•‘  â€¢ KRaft: Raft protocol handles network splits     â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ unclean.leader.election: Control consistency    â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ Survives datacenter failures with proper setup  â•‘ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â•‘  CONSISTENCY     â•‘  â€¢ acks=all: Strong consistency                    â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ acks=1: Leader-only consistency                 â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ acks=0: No consistency guarantee                â•‘ â”‚
â”‚   â•‘                  â•‘  â€¢ Exactly-once: Idempotent + transactional        â•‘ â”‚
â”‚   â•‘                  â•‘                                                    â•‘ â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Kafka's Distributed Architecture

> "Now let's see how Kafka implements all these distributed system qualities.
>
> Kafka runs as a cluster of brokers - independent servers that store and serve data. Your data is split into partitions, spread across these brokers. Each partition is replicated, so if one broker dies, another has a copy. The controller manages everything - which broker leads each partition, when to elect a new leader. With KRaft, this is all built into Kafka itself.
>
> For scalability, Kafka uses partitions to divide data across brokers. You can have consumers process partitions in parallel. LinkedIn handles over two million messages per second per broker.
>
> For fault tolerance, Kafka replicates each partition across multiple brokers. The ISR - In-Sync Replicas - tracks which copies are current. When a leader fails, a follower from the ISR takes over automatically.
>
> For high availability, there's no single point of failure. Brokers can be spread across data centers. Leader election happens in under a second with KRaft.
>
> For partition tolerance, Kafka uses quorum-based consensus. The controller quorum needs a majority to function, preventing split-brain scenarios.
>
> For consistency, the acks setting gives you control. Acks equals all for strong consistency, acks equals one for balance, acks equals zero for maximum speed."

---

## Part 5: Kafka as a Streaming Platform

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BATCH vs STREAM PROCESSING                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   BATCH PROCESSING (Traditional)                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚                                                                  â”‚      â”‚
â”‚   â”‚  Data Source â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚      â”‚
â”‚   â”‚                  â”‚  Collect data   â”‚                            â”‚      â”‚
â”‚   â”‚                  â”‚  for hours/days â”‚                            â”‚      â”‚
â”‚   â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚      â”‚
â”‚   â”‚                           â”‚                                     â”‚      â”‚
â”‚   â”‚                           â–¼                                     â”‚      â”‚
â”‚   â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚      â”‚
â”‚   â”‚                  â”‚  Process all    â”‚                            â”‚      â”‚
â”‚   â”‚                  â”‚  at once        â”‚                            â”‚      â”‚
â”‚   â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚      â”‚
â”‚   â”‚                           â”‚                                     â”‚      â”‚
â”‚   â”‚                           â–¼                                     â”‚      â”‚
â”‚   â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚      â”‚
â”‚   â”‚                  â”‚    Results      â”‚   (e.g., Daily report)     â”‚      â”‚
â”‚   â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚      â”‚
â”‚   â”‚                                                                  â”‚      â”‚
â”‚   â”‚   Examples: Hadoop MapReduce, Spark Batch, AWS Glue             â”‚      â”‚
â”‚   â”‚   Latency: Minutes to hours                                     â”‚      â”‚
â”‚   â”‚                                                                  â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   STREAM PROCESSING (Kafka)                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚                                                                  â”‚      â”‚
â”‚   â”‚   Event 1 â”€â”€â–¶ Process â”€â”€â–¶ Result 1                              â”‚      â”‚
â”‚   â”‚   Event 2 â”€â”€â–¶ Process â”€â”€â–¶ Result 2                              â”‚      â”‚
â”‚   â”‚   Event 3 â”€â”€â–¶ Process â”€â”€â–¶ Result 3                              â”‚      â”‚
â”‚   â”‚   Event 4 â”€â”€â–¶ Process â”€â”€â–¶ Result 4                              â”‚      â”‚
â”‚   â”‚      â”‚                                                          â”‚      â”‚
â”‚   â”‚      â–¼                                                          â”‚      â”‚
â”‚   â”‚   Continuous flow, each event processed as it arrives           â”‚      â”‚
â”‚   â”‚                                                                  â”‚      â”‚
â”‚   â”‚   Examples: Kafka Streams, Apache Flink, Apache Spark Streaming â”‚      â”‚
â”‚   â”‚   Latency: Milliseconds to seconds                              â”‚      â”‚
â”‚   â”‚                                                                  â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   COMPARISON:                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚                â”‚    BATCH           â”‚    STREAM (Kafka)          â”‚      â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚      â”‚
â”‚   â”‚ Data arrival   â”‚ Collected, stored  â”‚ Processed immediately      â”‚      â”‚
â”‚   â”‚ Latency        â”‚ Minutes to hours   â”‚ Milliseconds to seconds    â”‚      â”‚
â”‚   â”‚ Throughput     â”‚ Very high          â”‚ High (but prioritizes low  â”‚      â”‚
â”‚   â”‚                â”‚                    â”‚ latency)                   â”‚      â”‚
â”‚   â”‚ Complexity     â”‚ Simpler            â”‚ More complex (state mgmt)  â”‚      â”‚
â”‚   â”‚ Use case       â”‚ Analytics, reports â”‚ Real-time alerts, fraud    â”‚      â”‚
â”‚   â”‚ Recovery       â”‚ Rerun the batch    â”‚ Resume from offset         â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Makes Kafka Unique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA - MORE THAN A MESSAGE QUEUE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   TRADITIONAL MESSAGE QUEUE               KAFKA                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€                             â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚   â”‚Producer â”‚â”€â”€â–¶â”‚ Queue â”‚â”€â”€â–¶â”‚Consumer â”‚    â”‚Producer â”‚â”€â–¶â”‚ Kafka â”‚â”€â–¶â”‚Consumerâ”‚â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                     â”‚                                  â”‚       â”‚           â”‚
â”‚                     â–¼                                  â”‚       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚              Message DELETED                           â”‚       â”‚â”€â–¶â”‚Consumerâ”‚â”‚
â”‚              after consumption                         â”‚ LOG   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                        â”‚       â”‚           â”‚
â”‚                                                        â”‚       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚                                                        â”‚       â”‚â”€â–¶â”‚Consumerâ”‚â”‚
â”‚                                                        â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                            â”‚               â”‚
â”‚                                                            â–¼               â”‚
â”‚                                                    Message RETAINED        â”‚
â”‚                                                    (configurable duration) â”‚
â”‚                                                                             â”‚
â”‚   KEY DIFFERENCES:                                                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Feature           â”‚ Traditional Queue    â”‚ Kafka                    â”‚  â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚
â”‚   â”‚ Message storage   â”‚ Delete after consume â”‚ Retain (time/size based) â”‚  â”‚
â”‚   â”‚ Replay            â”‚ âœ— Not possible       â”‚ âœ“ Any consumer can replayâ”‚  â”‚
â”‚   â”‚ Multiple consumersâ”‚ Competing consumers  â”‚ Independent consumer     â”‚  â”‚
â”‚   â”‚                   â”‚ (one wins)           â”‚ groups (all get data)    â”‚  â”‚
â”‚   â”‚ Ordering          â”‚ Often not guaranteed â”‚ Guaranteed per partition â”‚  â”‚
â”‚   â”‚ Storage model     â”‚ Queue (FIFO, delete) â”‚ Log (append-only, retain)â”‚  â”‚
â”‚   â”‚ Throughput        â”‚ Thousands/sec        â”‚ Millions/sec             â”‚  â”‚
â”‚   â”‚ Typical latency   â”‚ Single-digit ms      â”‚ Single-digit ms          â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   KAFKA AS A DISTRIBUTED COMMIT LOG:                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    The Commit Log Concept                           â”‚   â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚   Think of it like a database's write-ahead log (WAL):              â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚   â”‚   â”‚  0  â”‚  1  â”‚  2  â”‚  3  â”‚  4  â”‚  5  â”‚  6  â”‚  7  â”‚  8  â”‚ ... â”‚    â”‚   â”‚
â”‚   â”‚   â”‚ Msg â”‚ Msg â”‚ Msg â”‚ Msg â”‚ Msg â”‚ Msg â”‚ Msg â”‚ Msg â”‚ Msg â”‚     â”‚    â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚   â”‚     â–²                               â–²                    â–²         â”‚   â”‚
â”‚   â”‚     â”‚                               â”‚                    â”‚         â”‚   â”‚
â”‚   â”‚   Consumer A                    Consumer B          New messages   â”‚   â”‚
â”‚   â”‚   (at offset 0)                (at offset 5)         appended      â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚   â€¢ Append-only: Never modify, only add                            â”‚   â”‚
â”‚   â”‚   â€¢ Offset-based: Each message has unique position                 â”‚   â”‚
â”‚   â”‚   â€¢ Consumers control their position: Read at own pace             â”‚   â”‚
â”‚   â”‚   â€¢ Replay: Reset offset to re-process data                        â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Kafka as Streaming Platform

> "What makes Kafka a streaming platform, not just a message queue?
>
> First, let's understand the difference between batch and stream processing. Batch processing collects data over time - hours or days - then processes it all at once. Think of a nightly report. Stream processing handles data as it arrives, continuously. Think of fraud detection - you need to catch suspicious transactions immediately, not at the end of the day.
>
> Traditional message queues delete messages after they're consumed. Kafka retains them. This is a fundamental difference that changes everything.
>
> With a traditional queue, once a consumer reads a message, it's gone. With Kafka, the message stays in the log. Multiple applications can read the same data independently. If a new application comes online, it can read all historical data. If something goes wrong, you can replay and reprocess.
>
> Think of Kafka as a distributed commit log - like a database's write-ahead log. Messages are appended to the end. Each has a unique offset. Consumers track their position and can move backward or forward. This model enables use cases that are impossible with traditional queues.
>
> And Kafka is fast. LinkedIn processes over seven trillion messages per day. Netflix over one trillion. This is where the distributed architecture pays off - partitions enable parallel processing across brokers and consumers."

---

## Part 6: Real-World Use Cases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA USE CASES                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   1. EVENT SOURCING / CQRS                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚                                                                             â”‚
â”‚   Every state change is stored as an immutable event                        â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚  Order Service                                                  â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  User Action              Event in Kafka         Current State  â”‚       â”‚
â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚       â”‚
â”‚   â”‚  Create order   â”€â”€â–¶   OrderCreated           â”€â”€â–¶  CREATED      â”‚       â”‚
â”‚   â”‚  Pay for order  â”€â”€â–¶   PaymentReceived        â”€â”€â–¶  PAID         â”‚       â”‚
â”‚   â”‚  Ship order     â”€â”€â–¶   OrderShipped           â”€â”€â–¶  SHIPPED      â”‚       â”‚
â”‚   â”‚  Deliver order  â”€â”€â–¶   OrderDelivered         â”€â”€â–¶  DELIVERED    â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  Can rebuild state by replaying all events!                     â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   2. REAL-TIME ANALYTICS                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  Website â”€â”€â–¶ Clickstream â”€â”€â–¶ Kafka â”€â”€â–¶ Flink â”€â”€â–¶ Dashboard      â”‚       â”‚
â”‚   â”‚              Events                                             â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  "100 users viewing product X right now"                        â”‚       â”‚
â”‚   â”‚  "Sales trending up 15% in the last hour"                       â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   3. FRAUD DETECTION                                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  Transaction â”€â”€â–¶ Kafka â”€â”€â–¶ ML Model â”€â”€â”¬â”€â”€â–¶ Approve              â”‚       â”‚
â”‚   â”‚                                       â”‚                         â”‚       â”‚
â”‚   â”‚                                       â””â”€â”€â–¶ Block + Alert        â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  Latency requirement: < 100ms                                   â”‚       â”‚
â”‚   â”‚  Kafka enables real-time scoring of every transaction           â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   4. MICROSERVICES COMMUNICATION                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚       â”‚
â”‚   â”‚  â”‚ Order   â”‚â”€â”€â”€â–¶â”‚           â”‚â”€â”€â”€â–¶â”‚Inventoryâ”‚                   â”‚       â”‚
â”‚   â”‚  â”‚ Service â”‚    â”‚           â”‚    â”‚ Service â”‚                   â”‚       â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   KAFKA   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚       â”‚
â”‚   â”‚                 â”‚           â”‚                                   â”‚       â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚           â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚       â”‚
â”‚   â”‚  â”‚ User    â”‚â”€â”€â”€â–¶â”‚           â”‚â”€â”€â”€â–¶â”‚ Email   â”‚                   â”‚       â”‚
â”‚   â”‚  â”‚ Service â”‚    â”‚           â”‚    â”‚ Service â”‚                   â”‚       â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  Services are decoupled. If Email Service is down,              â”‚       â”‚
â”‚   â”‚  messages wait in Kafka until it recovers.                      â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   5. LOG AGGREGATION                                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  Server 1 â”€â”                                                    â”‚       â”‚
â”‚   â”‚  Server 2 â”€â”¼â”€â”€â–¶ Kafka â”€â”€â–¶ Elasticsearch â”€â”€â–¶ Kibana Dashboard    â”‚       â”‚
â”‚   â”‚  Server 3 â”€â”¤             (or Splunk)                            â”‚       â”‚
â”‚   â”‚  ...      â”€â”˜                                                    â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚  Centralized logging for thousands of servers                   â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   WHO USES KAFKA AT SCALE?                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ Company      â”‚ Scale                                           â”‚       â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚   â”‚ LinkedIn     â”‚ 7+ trillion messages/day, 100+ PB storage       â”‚       â”‚
â”‚   â”‚ Netflix      â”‚ 1+ trillion messages/day                        â”‚       â”‚
â”‚   â”‚ Uber         â”‚ 1+ trillion messages/day                        â”‚       â”‚
â”‚   â”‚ Twitter      â”‚ Hundreds of billions messages/day               â”‚       â”‚
â”‚   â”‚ Spotify      â”‚ 4+ million events/second                        â”‚       â”‚
â”‚   â”‚ Airbnb       â”‚ Billions of events/day                          â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Use Cases

> "Let's look at real-world use cases for Kafka as a distributed streaming system.
>
> First, event sourcing. Instead of storing just the current state, you store every state change as an event. Order created, payment received, order shipped, order delivered. The current state is derived by replaying events. If you need to debug or audit, you have the complete history. This is powerful for compliance and analytics.
>
> Second, real-time analytics. Clickstream data flows through Kafka to a stream processor like Flink, then to a dashboard. You can see how many users are viewing a product right now, or that sales are trending up 15% in the last hour - not in yesterday's report, but right now.
>
> Third, fraud detection. Every transaction flows through Kafka to a machine learning model. The model scores it in under 100 milliseconds and either approves or blocks the transaction. This wouldn't be possible with batch processing.
>
> Fourth, microservices communication. Kafka decouples your services. When the order service creates an order, it publishes an event. The inventory service, email service, and analytics service all consume it independently. If one service is down, messages wait in Kafka until it recovers. No data loss, no tight coupling.
>
> Fifth, log aggregation. Thousands of servers send logs to Kafka. From there, they flow to Elasticsearch for search, to Hadoop for batch analytics, or to alerting systems. Kafka acts as a central buffer that handles the scale.
>
> Who uses Kafka at this scale? LinkedIn - where Kafka was created - processes over seven trillion messages per day. Netflix, Uber, Twitter - they're all processing trillions of messages. Spotify handles four million events per second. This is the power of a distributed streaming system."

---

## Part 7: Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SUMMARY: KAFKA AS A DISTRIBUTED STREAMING SYSTEM         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚   â•‘                                                                     â•‘   â”‚
â”‚   â•‘   DISTRIBUTED                        STREAMING                      â•‘   â”‚
â”‚   â•‘   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â•‘   â”‚
â”‚   â•‘                                                                     â•‘   â”‚
â”‚   â•‘   âœ“ Multiple brokers                 âœ“ Real-time processing         â•‘   â”‚
â”‚   â•‘   âœ“ Data partitioned                 âœ“ Continuous data flow         â•‘   â”‚
â”‚   â•‘   âœ“ Replicated for safety            âœ“ Low latency (ms)             â•‘   â”‚
â”‚   â•‘   âœ“ No single point of failure       âœ“ High throughput (M msg/sec)  â•‘   â”‚
â”‚   â•‘   âœ“ Horizontally scalable            âœ“ Exactly-once semantics       â•‘   â”‚
â”‚   â•‘                                                                     â•‘   â”‚
â”‚   â•‘                     DURABLE STORAGE                                 â•‘   â”‚
â”‚   â•‘                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â•‘   â”‚
â”‚   â•‘                                                                     â•‘   â”‚
â”‚   â•‘                     âœ“ Commit log (not just queue)                   â•‘   â”‚
â”‚   â•‘                     âœ“ Configurable retention                        â•‘   â”‚
â”‚   â•‘                     âœ“ Replay capability                             â•‘   â”‚
â”‚   â•‘                     âœ“ Multiple independent consumers                â•‘   â”‚
â”‚   â•‘                                                                     â•‘   â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   KAFKA IN THE CAP THEOREM:                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚                                                                             â”‚
â”‚   Kafka is a CP system by default:                                          â”‚
â”‚   â€¢ Prioritizes Consistency over Availability                               â”‚
â”‚   â€¢ During network partitions, may reject writes to maintain consistency    â”‚
â”‚   â€¢ Configurable: You can tune towards AP with relaxed settings             â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚   WHY KAFKA DOMINATES:                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚   "Kafka combines the best of messaging systems and databases"  â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚   â€¢ Message Queue: Pub/sub, decouple producers/consumers        â”‚       â”‚
â”‚   â”‚   â€¢ Database: Durable storage, replay, exactly-once             â”‚       â”‚
â”‚   â”‚   â€¢ Stream Processor: Real-time transformations (Kafka Streams) â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â”‚   All in one distributed, fault-tolerant, scalable platform!    â”‚       â”‚
â”‚   â”‚                                                                 â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Script - Summary

> "Let's summarize what we've learned about Kafka as a distributed streaming system.
>
> On the distributed side: Kafka runs as a cluster of multiple brokers. Data is partitioned across them for parallelism. Replicated for safety. There's no single point of failure. And it scales horizontally - just add more brokers.
>
> On the streaming side: Kafka processes data in real-time with millisecond latency. It handles millions of messages per second. It supports exactly-once semantics for critical applications.
>
> And uniquely, Kafka provides durable storage. It's not just a pipe passing messages through. It's a distributed commit log that retains data. You can replay, you can have multiple independent consumers, you can rebuild state from history.
>
> In the CAP theorem, Kafka is a CP system. It prioritizes consistency over availability. During network partitions, Kafka may reject writes rather than risk inconsistency. This is the right choice for most enterprise use cases.
>
> Why does Kafka dominate? Because it combines the best of messaging systems - pub/sub and decoupling - with the best of databases - durable storage and replay - and adds stream processing on top. All in one distributed, fault-tolerant, scalable platform.
>
> In the next lecture, we'll dive into the commit log in detail and see exactly how Kafka stores and retrieves your data."

---

## Diagram

![KRaft Single Broker Flow](kraft-single-broker-flow.png)
